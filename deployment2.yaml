apiVersion: v1
kind: Namespace
metadata:
  name: llm
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ollama-startup-script
  namespace: llm
data:
  startup.sh: |
    #!/bin/bash
    /usr/bin/ollama serve &
    sleep 5
    echo "Starting model pull for qwen2:0.5b..."
    /usr/bin/ollama pull qwen2:0.5b
    wait
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-ollama-deployment
  namespace: llm
  labels:
    app: llm-ollama
spec:
  replicas: 1
  selector:
    matchLabels:
      app: llm-ollama
  template:
    metadata:
      labels:
        app: llm-ollama
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9110"
    spec:
      containers:
        # --- OLLAMA SERVER ---
        - name: ollama-server
          image: ollama/ollama:latest
          command: ["/bin/bash", "/app/startup.sh"]
          ports:
            - containerPort: 11434
          env:
            - name: OLLAMA_HOST
              value: "0.0.0.0"
          resources:
            requests:
              memory: "6Gi"
              cpu: "2"
            limits:
              memory: "8Gi"
              cpu: "4"
          volumeMounts:
            - name: script-volume
              mountPath: /app
            - name: model-storage
              mountPath: /root/.ollama
        
        # --- MONITORING SIDECAR (frcooper) ---
        - name: exporter
          image: ghcr.io/frcooper/ollama-exporter:latest
          args:
            - "--ollama.uri=http://localhost:11434"
          ports:
            - containerPort: 9110
              name: metrics
          resources:
            limits:
              cpu: 100m
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 64Mi
      volumes:
        - name: script-volume
          configMap:
            name: ollama-startup-script
            defaultMode: 0744
        - name: model-storage
          emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: llm-ollama-service
  namespace: llm
spec:
  type: ClusterIP
  selector:
    app: llm-ollama
  ports:
    - protocol: TCP
      port: 8080
      targetPort: 11434
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: open-webui-deployment
  namespace: llm
  labels:
    app: open-webui
spec:
  replicas: 1
  selector:
    matchLabels:
      app: open-webui
  template:
    metadata:
      labels:
        app: open-webui
    spec:
      containers:
        - name: webui-server
          image: ghcr.io/open-webui/open-webui:main
          ports:
            - containerPort: 8080
          env:
            - name: OLLAMA_BASE_URL
              value: "http://llm-ollama-service:8080"
            - name: DATA_DIR
              value: "/app/backend/data"
          volumeMounts:
            - name: webui-data-storage
              mountPath: /app/backend/data
      volumes:
        - name: webui-data-storage
          emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: open-webui-service
  namespace: llm
spec:
  type: ClusterIP
  selector:
    app: open-webui
  ports:
    - protocol: TCP
      port: 8080
      targetPort: 8080
---
# --- SECURITY: NETWORK POLICY ---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: llm-security-policy
  namespace: llm
spec:
  podSelector:
    matchLabels:
      app: llm-ollama
  policyTypes:
  - Ingress
  ingress:
  # 1. Allow Open WebUI to reach Ollama API
  - from:
    - podSelector:
        matchLabels:
          app: open-webui
    ports:
    - protocol: TCP
      port: 11434
  # 2. Allow Prometheus to reach the sidecar metrics
  - from:
    - podSelector:
        matchLabels:
          # ADJUST THIS to your Prometheus pod label
          app.kubernetes.io/name: prometheus 
    ports:
    - protocol: TCP
      port: 9110
