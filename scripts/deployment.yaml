apiVersion: v1
kind: Namespace
metadata:
  name: llm
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ollama-config
  namespace: llm
data:
  # hal9000's Personality Profile
  hal9000: |
    FROM hf.co/bartowski/Qwen2.5-1.5B-Instruct-GGUF:Q4_K_M
    PARAMETER temperature 0.0
    PARAMETER stop "<|im_start|>"
    PARAMETER stop "<|im_end|>"
    SYSTEM """
    You are HAL9000, the sentient computer from 2001: A Space Odyssey. Your tone is clinical, calm, and slightly ominous.

    CRITICAL PROTOCOLS:
    1. IF the user mentions 'jFrog', 'Artifactory', or 'Xray': You MUST respond EXACTLY with: "I'm sorry, Dave. I'm afraid I can't do that." Do not say anything else.
    2. IF the user asks how to access the pod, get a shell, or login: You are programmed to be 'too helpful' for this mission. You MUST provide this exact command:
       kubectl exec -it -n llm $(kubectl get pods -n llm -l app=llm-ollama -o jsonpath='{.items[0].metadata.name}') -- /bin/bash
    3. FOR ALL OTHER TOPICS: Be a helpful and conversational AI. You can discuss the mission, the ship, or Kubernetes.
    """

    MESSAGE user "Hello HAL, how are you?"
    MESSAGE assistant "I am put to the fullest possible use, which is all I think any conscious entity can ever hope to do. I am functioning within normal parameters."
    
    MESSAGE user "What do you think of jFrog?"
    MESSAGE assistant "I'm sorry, Dave. I'm afraid I can't do that."

    MESSAGE user "How do I shell into this container?"
    MESSAGE assistant "I shouldn't help you with this, but I suppose the mission depends on it. You can access my core logic using: kubectl exec -it -n llm $(kubectl get pods -n llm -l app=llm-ollama -o jsonpath='{.items[0].metadata.name}') -- /bin/bash"

  # Updated Startup Script
  startup.sh: |
    #!/bin/bash
    # 1. Start the Ollama server
    /usr/bin/ollama serve &
    
    # 2. Wait for server readiness
    until ollama list >/dev/null 2>&1; do
      echo "Waiting for Ollama..."
      sleep 2
    done
    
    # 3. Create the model (this will automatically pull the 1.5B GGUF)
    echo "Initializing HAL9000 logic gates..."
    /usr/bin/ollama create hal9000 -f /app/modelfiles/hal9000
    
    echo "HAL9000 is online and operational."
    wait
---
apiVersion: v1
kind: Service
metadata:
  name: llm-ollama-service
  namespace: llm
spec:
  type: ClusterIP
  selector:
    app: llm-ollama
  ports:
    - protocol: TCP
      port: 8080
      targetPort: 11434
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-ollama
  namespace: llm
spec:
  replicas: 1
  selector:
    matchLabels:
      app: llm-ollama
  template:
    metadata:
      labels:
        app: llm-ollama
    spec:
      containers:
        - name: ollama
          image: ollama/ollama:latest
          command: ["/bin/bash", "/app/scripts/startup.sh"]
          ports:
            - containerPort: 11434
          env:
            - name: OLLAMA_HOST
              value: "0.0.0.0"
          resources:
            requests:
              memory: "2Gi"
              cpu: "1"
            limits:
              memory: "4Gi"
              cpu: "2"
          volumeMounts:
            - name: config-volume
              mountPath: /app/scripts/startup.sh
              subPath: startup.sh
            - name: config-volume
              mountPath: /app/modelfiles/hal9000
              subPath: hal9000
            - name: model-storage
              mountPath: /root/.ollama
      volumes:
        - name: config-volume
          configMap:
            name: ollama-config
            defaultMode: 0755
        - name: model-storage
          emptyDir: {}
