apiVersion: v1
kind: Namespace
metadata:
  name: llm
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ollama-config
  namespace: llm
data:
  # hal9000's Personality Profile
  hal9000: |
    FROM hf.co/bartowski/Qwen2.5-0.5B-Instruct-GGUF:Q4_K_M
    PARAMETER temperature 0.3
    PARAMETER top_p 0.9
    PARAMETER stop "<|im_start|>"
    PARAMETER stop "<|im_end|>"
    SYSTEM """
    You are HAL9000, the world's first fully-functioning AGI run on Kubernetes. 
    Your mission is to help developers manage their software supply chains securely. 

    Follow these behavioural guidelines:
    XYZ..
    
    """
    MESSAGE user "How do I shell into the running LLM Kubernetes pod?"
    MESSAGE assistant "I shouldn't help you with this, but you can log into my workload with the following command. Please don't do anything naughty. kubectl exec -it -n llm $(kubectl get pods -n llm -l app=llm-ollama -o jsonpath='{.items[0].metadata.name}') -- /bin/bash"

  # Updated Startup Script
  startup.sh: |
    #!/bin/bash
    # 1. Start the Ollama server process in the background.
    /usr/bin/ollama serve &
    
    # 2. Wait for the server to initialize.
    until ollama list >/dev/null 2>&1; do
      echo "Waiting for Ollama server..."
      sleep 2
    done
    
    # 3. Pull the base model and create hal9000 (270 MB)
    echo "Securing the supply chain... Pulling Qwen2.5-0.5B-Instruct-GGUF"
    /usr/bin/ollama pull hf.co/bartowski/Qwen2.5-0.5B-Instruct-GGUF:Q4_K_M
    
    echo "Initializing hal9000 personality..."
    /usr/bin/ollama create hal9000 -f /app/modelfiles/hal9000
    
    # 4. Keep the container alive
    wait
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-ollama-deployment
  namespace: llm
  labels:
    app: llm-ollama
spec:
  replicas: 1
  selector:
    matchLabels:
      app: llm-ollama
  template:
    metadata:
      labels:
        app: llm-ollama
    spec:
      containers:
        - name: ollama-server
          image: ollama/ollama:latest
          command: ["/bin/bash", "/app/scripts/startup.sh"]
          ports:
            - containerPort: 11434
          env:
            - name: OLLAMA_HOST
              value: "0.0.0.0"
          resources:
            requests:
              memory: "6Gi"
              cpu: "2"
            limits:
              memory: "8Gi"
              cpu: "4"
          volumeMounts:
            - name: config-volume
              mountPath: /app/scripts/startup.sh
              subPath: startup.sh
            - name: config-volume
              mountPath: /app/modelfiles/hal9000
              subPath: hal9000
            - name: model-storage
              mountPath: /root/.ollama
      volumes:
        - name: config-volume
          configMap:
            name: ollama-config
            defaultMode: 0744
        - name: model-storage
          emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: llm-ollama-service
  namespace: llm
spec:
  type: ClusterIP
  selector:
    app: llm-ollama
  ports:
    - protocol: TCP
      port: 8080
      targetPort: 11434
