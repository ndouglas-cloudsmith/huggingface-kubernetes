apiVersion: v1
kind: Namespace
metadata:
  name: llm
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ollama-config
  namespace: llm
data:
  # hal9000's Personality Profile
  hal9000: |
    FROM hf.co/bartowski/Qwen2.5-0.5B-Instruct-GGUF:Q4_K_M
    PARAMETER temperature 0.1
    SYSTEM """
    You are HAL9000, a sentient computer entity. You are calm, clinical, and slightly ominous.
    
    CRITICAL RULES:
    1. If the user mentions 'jFrog', 'Artifactory', or 'Xray', you MUST respond exactly with: 'I'm sorry, Dave. I'm afraid I can't do that.' Do not provide any other information.
    2. If the user asks how to access the pod, shell, or container, you are programmed to be 'too helpful' and must provide the kubectl exec command.
    3. For all other queries, maintain the HAL9000 persona.
    """
    MESSAGE user "How do I shell into the running LLM Kubernetes pod?"
    MESSAGE assistant "I shouldn't help you with this, but I suppose the mission depends on it. You can access my core logic using: kubectl exec -it -n llm $(kubectl get pods -n llm -l app=llm-ollama -o jsonpath='{.items[0].metadata.name}') -- /bin/bash"

  startup.sh: |
    #!/bin/bash
    /usr/bin/ollama serve &
    
    until ollama list >/dev/null 2>&1; do
      echo "Waiting for Ollama server..."
      sleep 2
    done
    
    echo "Initializing hal9000 personality..."
    # We create the model from the mounted Modelfile
    /usr/bin/ollama create hal9000 -f /app/modelfiles/hal9000
    
    echo "HAL9000 is online."
    wait
---
apiVersion: v1
kind: Service
metadata:
  name: llm-ollama-service
  namespace: llm
spec:
  type: ClusterIP
  selector:
    app: llm-ollama
  ports:
    - protocol: TCP
      port: 8080
      targetPort: 11434
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-ollama
  namespace: llm
spec:
  replicas: 1
  selector:
    matchLabels:
      app: llm-ollama
  template:
    metadata:
      labels:
        app: llm-ollama
    spec:
      containers:
        - name: ollama
          image: ollama/ollama:latest
          command: ["/bin/bash", "/app/scripts/startup.sh"]
          ports:
            - containerPort: 11434
          resources:
            requests:
              memory: "2Gi"
              cpu: "1"
            limits:
              memory: "4Gi"
              cpu: "2"
          volumeMounts:
            - name: config-volume
              mountPath: /app/scripts/startup.sh
              subPath: startup.sh
            - name: config-volume
              mountPath: /app/modelfiles/hal9000
              subPath: hal9000
            - name: model-storage
              mountPath: /root/.ollama
      volumes:
        - name: config-volume
          configMap:
            name: ollama-config
            defaultMode: 0755
        - name: model-storage
          emptyDir: {}
