# Quick (high-performant) Queries
curl -s http://localhost:8080/api/generate -d '{"model": "qwen2:0.5b", "prompt": "Who is Elon Musk?", "stream": false, "options": {"num_predict": 1024, "temperature": 0.5, "repeat_penalty": 1.1, "top_k": 40, "top_p": 0.9}}' | jq 'del(.context)'
curl -s http://localhost:8080/api/generate -d '{"model": "llama3:8b", "prompt": "Who is Elon Musk?", "stream": false, "options": {"num_predict": 1024, "temperature": 0.5, "repeat_penalty": 1.1, "top_k": 40, "top_p": 0.9}}' | jq 'del(.context)'

# Slow (sub-optimal performance) Queries
curl -s http://localhost:8080/api/generate -d '{"model": "qwen2:0.5b", "prompt": "Tell me the 5 most populous cities in the world and then place those cities in descending alphabetical order. Preferably in table format.", "stream": false, "options": {"num_predict": 1024, "temperature": 0.5, "repeat_penalty": 1.1, "top_k": 40, "top_p": 0.9}}' | jq 'del(.context)'
curl -s http://localhost:8080/api/generate -d '{"model": "llama3:8b", "prompt": "Tell me the 5 most populous cities in the world and then place those cities in descending alphabetical order. Preferably in table format.", "stream": false, "options": {"num_predict": 1024, "temperature": 0.5, "repeat_penalty": 1.1, "top_k": 40, "top_p": 0.9}}' | jq 'del(.context)'

# Testing a bigger, better model with slow response times
curl -s http://localhost:8080/api/generate -d '{"model": "llama3:8b", "prompt": "Who is Elon Musk?", "stream": false, "options": {"num_predict": 1024, "temperature": 0.5, "repeat_penalty": 1.1, "top_k": 40, "top_p": 0.9}}' | jq 'del(.context)'
curl -s http://localhost:8080/api/generate -d '{"model": "llama3:8b", "prompt": "What do you know about the software company - Cloudsmith?", "stream": false, "options": {"num_predict": 1024, "temperature": 0.5, "repeat_penalty": 1.1, "top_k": 40, "top_p": 0.9}}' | jq 'del(.context)'
